{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagri32/Analyzing-the-Extent-of-Polarization-around-COVID-19-Policies-using-Social-Media/blob/main/Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfoWPoVH1nw-"
      },
      "source": [
        "\n",
        "tutorial.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1CU6bPzyd1dU9i1ye6-nuci_2WNy60zaZ\n",
        "\n",
        "# **COVID-19 Dataset - How to use it?**\n",
        "\n",
        "## ***Background***\n",
        "\n",
        "Due to the relevance of the COVID-19 global pandemic, we are releasing our dataset of tweets acquired from the Twitter Stream related to COVID-19 chatter. \n",
        "\n",
        "The data collected from the stream captures all languages, but the higher prevalence are:  English, Spanish, and French. We release all tweets and retweets on the full_dataset.tsv file, and a cleaned version with no retweets on the full_dataset-clean.tsv file .\n",
        "\n",
        "The main repository for this dataset (and latest version) can be found here https://doi.org/10.5281/zenodo.3723939\n",
        "\n",
        "Intermediate bi-weekly updates are posted here: https://github.com/thepanacealab/covid19_twitter\n",
        "\n",
        "As always, the tweets distributed here are only tweet identifiers (with date and time added) due to the terms and conditions of Twitter to re-distribute Twitter data. They need to be hydrated to be used.\n",
        "\n",
        "If you are using our dataset, please cite our preprint:\n",
        "https://arxiv.org/abs/2004.03688\n",
        "\n",
        "## ***Introduction***\n",
        "\n",
        "\n",
        "In this tutorial, we will explain in a clear and detailed way how to use the data sets generated from this repository (https://github.com/thepanacealab/covid19_twitter/tree/master/dailies). It will explain how the process of hydration of tweets is done, the process of parsing, and an example that consists of counting the unique words of a certain dataset of tweets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWNGJMucnirC"
      },
      "source": [
        "Requirements\n",
        "First, we are going to install the following modules:\n",
        "\n",
        "Twarc\n",
        "Tweepy (v. 3.8.0)\n",
        "Argparse (v 3.2)\n",
        "Xtract (v 0.1 a3)\n",
        "Wget (v 3.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVb1n6AMnlD1"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install twarc #Twarc\n",
        "!pip install tweepy # Tweepy 3.8.0\n",
        "!pip install argparse #Argparse 3.2\n",
        "!pip install xtract #Xtract 0.1 a3\n",
        "!pip install wget #Wget 3.2\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yws9X2j1nntB"
      },
      "source": [
        "Selecting the dataset and language\n",
        "The dataset used for this tutorial was downloaded from here: https://github.com/thepanacealab/covid19_twitter/blob/master/dailies/2020-07-13/2020-07-13-dataset.tsv.gz?raw=true\n",
        "\n",
        "More datasets can be obtained from here: https://github.com/thepanacealab/covid19_twitter/tree/master/dailies\n",
        "\n",
        "The structure of the dataset is made up of the following fields:\n",
        "\n",
        "tweet_id The integer representation of the unique identifier for this Tweet.\n",
        "date Date when the tweet was posted (YYYY-MM-DD)\n",
        "time Time when the tweet was posted (HH:mm:ss)\n",
        "lang Language in which the text is written. Represented by a 2-character language code. If language is unknown, the value will be shown as 'und' (undefined)\n",
        "country_code Two character string representing the country where the tweet was written. If not known, the field will show as NULL\n",
        "Filtering a dataset from a language is done by specifying the language code. More information about language codes can be found here:\n",
        "\n",
        "https://developer.twitter.com/en/docs/twitter-for-websites/supported-languages\n",
        "\n",
        "In this example,we are going to filter the dataset, so we can only obtain tweets in spanish (So, that means we are going to use the language code \"es\")\n",
        "\n",
        "IMPORTANT: In this tutorial, after running the following code, please select the desired language to filter from the dropdown field (which is shown in the output code). If we don't want to filter the dataset, just select \"all\" in the dropdown field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "89acb5f046a64671a160fc743f4d9a1e",
            "bba3191a72d248579708daf78e1e52fe",
            "6498a2ae54ca4729bae2e0f9180e3f86"
          ]
        },
        "id": "PpPYxQ1aGfwv",
        "outputId": "670dd2ea-c5e2-4f1d-eae4-fd7d163b97ef"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89acb5f046a64671a160fc743f4d9a1e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dropdown(index=13, options=('all', 'am', 'ar', 'bg', 'bn', 'ca', 'ckb', 'cs', 'cy', 'da', 'de', 'dv', 'el', 'e…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import gzip\n",
        "import shutil\n",
        "import os\n",
        "import wget\n",
        "import csv\n",
        "import linecache\n",
        "from shutil import copyfile\n",
        "import ipywidgets as widgets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "date = \"2020-09-10\"\n",
        "dataset_URL = \"https://github.com/thepanacealab/covid19_twitter/blob/master/dailies/{}/{}-dataset.tsv.gz?raw=true\".format(date,date) #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "#Downloads the dataset (compressed in a GZ format)\n",
        "#!wget dataset_URL -O dataset.tsv.gz\n",
        "wget.download(dataset_URL, out='dataset.tsv.gz')\n",
        "\n",
        "#Unzips the dataset and gets the TSV dataset\n",
        "with gzip.open('dataset.tsv.gz', 'rb') as f_in:\n",
        "    with open('dataset-{}.tsv'.format(date), 'wb') as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "#Deletes the compressed GZ file\n",
        "os.unlink(\"dataset.tsv.gz\")\n",
        "\n",
        "#Gets all possible languages from the dataset\n",
        "df = pd.read_csv('dataset-{}.tsv'.format(date),sep=\"\\t\")\n",
        "lang_list = df.lang.unique()\n",
        "lang_list= sorted(np.append(lang_list,'all'))\n",
        "lang_picker = widgets.Dropdown(options=lang_list, value=\"en\")\n",
        "lang_picker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRpHl_8DnvDF"
      },
      "source": [
        "Dropdown(options=('all', 'am', 'ar', 'bg', 'bn', 'bo', 'ca', 'ckb', 'cs', 'cy', 'da', 'de', 'dv', 'el', 'en', …\n",
        "Filtering the dataset by language\n",
        "After selecting the desired language, the following code will perform the corresponding filtering to show only the records in the dataset that have the selected language (in a new tsv file called dataset-filtered.tsv). If there's no language filter, no filter process will be taken (but the file name will be dataset-filtered.tsv anyways)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RnI00fRnwfS",
        "outputId": "0eff57e1-c115-467e-ad68-75e63c6bf586"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mShowing first 5 tweets from the filtered dataset\u001b[0m\n",
            "['1303906699888201729\\t2020-09-10\\t04:02:37\\ten\\tNULL\\n', '1303906700114698240\\t2020-09-10\\t04:02:37\\ten\\tNULL\\n', '1303906700165087232\\t2020-09-10\\t04:02:37\\ten\\tNULL\\n', '1303906700169109505\\t2020-09-10\\t04:02:37\\ten\\tNULL\\n', '1303906700286623749\\t2020-09-10\\t04:02:37\\ten\\tNULL\\n']\n"
          ]
        }
      ],
      "source": [
        "#Creates a new clean dataset with the specified language (if specified)\n",
        "filtered_language = lang_picker.value\n",
        "\n",
        "#If no language specified, it will get all records from the dataset\n",
        "if filtered_language == \"\":\n",
        "  copyfile('dataset-{}.tsv'.format(date), 'dataset-filtered-{}.tsv'.format(date)) # change all 'dataset.tsv' with 'dataset-{}.tsv'.format(date)\n",
        "\n",
        "#If language specified, it will create another tsv file with the filtered records\n",
        "else:\n",
        "  filtered_tw = list()\n",
        "  current_line = 1\n",
        "  with open('dataset-{}.tsv'.format(date)) as tsvfile:\n",
        "    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
        "\n",
        "    if current_line == 1:\n",
        "      filtered_tw.append(linecache.getline('dataset-{}.tsv'.format(date), current_line))\n",
        "\n",
        "      for line in tsvreader:\n",
        "        if line[3] == filtered_language:\n",
        "          filtered_tw.append(linecache.getline('dataset-{}.tsv'.format(date), current_line))\n",
        "        current_line += 1\n",
        "\n",
        "  print('\\033[1mShowing first 5 tweets from the filtered dataset\\033[0m')\n",
        "  print(filtered_tw[1:(6 if len(filtered_tw) > 6 else len(filtered_tw))])\n",
        "\n",
        "  with open('dataset-filtered-{}.tsv'.format(date), 'w') as f_output:\n",
        "      for item in filtered_tw:\n",
        "          f_output.write(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2grRTsVnzCw"
      },
      "source": [
        "Showing first 5 tweets from the filtered dataset\n",
        "['1351757472873197568\\t2021-01-20\\t05:04:30\\tes\\tNULL\\n', '1351757472965353477\\t2021-01-20\\t05:04:30\\tes\\tNULL\\n', '1351757481723240448\\t2021-01-20\\t05:04:32\\tes\\tNULL\\n', '1351757481727455234\\t2021-01-20\\t05:04:32\\tes\\tNULL\\n', '1351757488706760705\\t2021-01-20\\t05:04:34\\tes\\tNULL\\n']\n",
        "Introducing our Twitter credentials to authenticate\n",
        "Accessing the Twitter APIs requires a set of credentials that you must pass with each request. These credentials can come in different forms depending on the type of authentication that is required by the specific endpoint that you are using. More information: https://developer.twitter.com/en/docs/apps/overview\n",
        "\n",
        "The credentials can be obtained from the developer portal (https://developer.twitter.com/en/portal/dashboard) and they look like these ones:\n",
        "\n",
        "IMPORTANT: The following code will also generate an api_keys.json (With the twitter credentials entered) that will be required later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCOfyi7yn3cv"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import tweepy\n",
        "from tweepy import OAuthHandler\n",
        "\n",
        "# Authenticate\n",
        "CONSUMER_KEY = \"\" #@param {type:\"string\"}\n",
        "CONSUMER_SECRET_KEY = \"\" #@param {type:\"string\"}\n",
        "ACCESS_TOKEN_KEY = \"\" #@param {type:\"string\"}\n",
        "ACCESS_TOKEN_SECRET_KEY = \"\" #@param {type:\"string\"}\n",
        "BEARER_TOKEN_KEY = \"AAAAAAAAAAAAAAAAAAAAAP0KUwEAAAAAh1pptk4pHUxP8LSHokl3DBiVxQg%3D1P0Ht0G0Jf4fAOUSxQxcFGzvExnF9xUqwkyBNmWPjgOJlbmamk\"\n",
        "#Creates a JSON Files with the API credentials\n",
        "with open('api_keys.json', 'w') as outfile:\n",
        "    json.dump({\n",
        "    \"consumer_key\":CONSUMER_KEY,\n",
        "    \"consumer_secret\":CONSUMER_SECRET_KEY,\n",
        "    \"access_token\":ACCESS_TOKEN_KEY,\n",
        "    \"access_token_secret\": ACCESS_TOKEN_SECRET_KEY,\n",
        "    \"bearer_token\": BEARER_TOKEN_KEY\n",
        "     }, outfile)\n",
        "\n",
        "#The lines below are just to test if the twitter credentials are correct\n",
        "#Authenticate\n",
        "auth = tweepy.AppAuthHandler(CONSUMER_KEY, CONSUMER_SECRET_KEY)\n",
        "\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True,\n",
        "\t\t\t\t   wait_on_rate_limit_notify=True)\n",
        "\n",
        "if (not api):\n",
        "   print (\"Can't Authenticate\")\n",
        "   sys.exit(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t0drxH5n5S_"
      },
      "source": [
        "Hydrating the tweets (filtered dataset)\n",
        "Before parsing the dataset, an hydration process is required. In this tutorial it is done by using the following social media mining tool: https://github.com/thepanacealab/SMMT\n",
        "\n",
        "To perform this action, a python file from that repository is required (get_metadata.py) and can be obtained in the following way:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9GXnNtFzbMB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0ACCEFon6uC"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "# Get the required files from GitHub\n",
        "!wget https://raw.githubusercontent.com/cagri32/Analyzing-the-Extent-of-Polarization-around-COVID-19-Policies-using-Social-Media/main/code/get_metadata.py -O get_metadata.py\n",
        "!wget https://raw.githubusercontent.com/cagri32/Analyzing-the-Extent-of-Polarization-around-COVID-19-Policies-using-Social-Media/main/code/main.py -O main.py\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4A4eL9mn7_X"
      },
      "source": [
        "This utility will take a file which meets the following requirements:\n",
        "*   A csv file which either contains one tweet id per line or contains at least one column of tweet ids\n",
        "*   A text file which contains one tweet id per line\n",
        "*   A tsv file which either contains one tweet id per line or contains at least one column of tweet ids\n",
        "\n",
        "For this case, the filtered dataset generated before (dataset-filtered.tsv), which is in TSV format will be used for the hydration process\n",
        "\n",
        "The arguments for this utily (get_metadata.py) are the following:\n",
        "\n",
        "*  arguments.png\n",
        "\n",
        "**PLEASE NOTE**: The -k argument refers to the json file with the Twitter credentials generated before (api_keys.json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5b5zfgU0AYG"
      },
      "outputs": [],
      "source": [
        "dataset_filtered_input = \"dataset-filtered-{}.tsv\".format(date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NNff2xHdoEZQ"
      },
      "outputs": [],
      "source": [
        "!python get_metadata.py -i {dataset_filtered_input} -o hydrated_tweets -k api_keys.json -ll 1000000 -ul 1500000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OWish5bJ1Lnk"
      },
      "outputs": [],
      "source": [
        "!python main.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vh81dbB5Clf"
      },
      "source": [
        "Test if we can bring a graph from a file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fR7yILl3E3Y",
        "outputId": "737714ab-2156-4eee-9cd1-372f691e4d75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DiGraph with 172877 nodes and 253312 edges\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"graph.txt\", 'rb') as f:  # notice the r instead of w\n",
        "    G_loaded = pickle.load(f)\n",
        "print(G_loaded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEMrJGxCHOIk"
      },
      "source": [
        "Download files and then delete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSRx1PV6swP3"
      },
      "outputs": [],
      "source": [
        "#@title Utility to zip and download a directory\n",
        "#@markdown Use this method to zip and download a directory. For ex. a TB logs \n",
        "#@markdown directory or a checkpoint(s) directory.\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "dir_to_zip = '/content' #@param {type: \"string\"}\n",
        "output_filename = \"hydrated-0-500.zip\".format()\n",
        "delete_dir_after_download = \"No\"  #@param ['Yes', 'No']\n",
        "\n",
        "os.system( \"zip -r {} {}\".format( output_filename , dir_to_zip ) )\n",
        "\n",
        "if delete_dir_after_download == \"Yes\":\n",
        "    os.system( \"rm -r {}\".format( dir_to_zip ) )\n",
        "\n",
        "files.download( output_filename )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yJ_ulZHfXhC"
      },
      "outputs": [],
      "source": [
        "# files.download( \"hydrated-all.zip\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DanQAiFR44g3"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# files.download(\"hydrated_tweets.zip\")\n",
        "# files.download(\"hydrated_tweets.csv\")\n",
        "# files.download(\"hydrated_tweets_short.json\")\n",
        "# files.download(\"hydrated_tweets\")\n",
        "os.unlink(\"hydrated_tweets\")\n",
        "os.unlink(\"hydrated_tweets.zip\")\n",
        "os.unlink(\"hydrated_tweets.csv\")\n",
        "os.unlink(\"hydrated_tweets_short.json\")\n",
        "os.unlink(\"graph.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI-9saZgoIzi"
      },
      "source": [
        "From the code above, the output will generate four files:\n",
        "\n",
        "*  A hydrated_tweets.json file which contains the full json object for each of the hydrated tweets\n",
        "*  A hydrated_tweets.CSV file which contains partial fields extracted from the tweets.\n",
        "*  A hydrated_tweets.zip file which contains a zipped version of the tweets_full.json file.\n",
        "*  A hydrated_tweets_short.json which contains a shortened version of the hydrated tweets.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Tutorial",
      "provenance": [],
      "authorship_tag": "ABX9TyP5R1SnCU9rdC3gyrSvXAGt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6498a2ae54ca4729bae2e0f9180e3f86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89acb5f046a64671a160fc743f4d9a1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DropdownModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "all",
              "am",
              "ar",
              "bg",
              "bn",
              "ca",
              "ckb",
              "cs",
              "cy",
              "da",
              "de",
              "dv",
              "el",
              "en",
              "es",
              "et",
              "eu",
              "fa",
              "fi",
              "fr",
              "gu",
              "hi",
              "ht",
              "hu",
              "hy",
              "in",
              "is",
              "it",
              "iw",
              "ja",
              "ka",
              "km",
              "kn",
              "ko",
              "lo",
              "lt",
              "lv",
              "ml",
              "mr",
              "my",
              "ne",
              "nl",
              "no",
              "or",
              "pa",
              "pl",
              "ps",
              "pt",
              "ro",
              "ru",
              "sd",
              "si",
              "sl",
              "sr",
              "sv",
              "ta",
              "te",
              "th",
              "tl",
              "tr",
              "uk",
              "und",
              "ur",
              "vi",
              "zh"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "index": 13,
            "layout": "IPY_MODEL_bba3191a72d248579708daf78e1e52fe",
            "style": "IPY_MODEL_6498a2ae54ca4729bae2e0f9180e3f86"
          }
        },
        "bba3191a72d248579708daf78e1e52fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}