{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNpp/KYO9iQlIkEEELotnC0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "77158db182fa4525bbbbd4c7572dfbe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_options_labels": [
              "all",
              "am",
              "ar",
              "bg",
              "bn",
              "ca",
              "ckb",
              "cs",
              "cy",
              "da",
              "de",
              "dv",
              "el",
              "en",
              "es",
              "et",
              "eu",
              "fa",
              "fi",
              "fr",
              "gu",
              "hi",
              "ht",
              "hu",
              "hy",
              "in",
              "is",
              "it",
              "iw",
              "ja",
              "ka",
              "km",
              "kn",
              "ko",
              "lo",
              "lt",
              "lv",
              "ml",
              "mr",
              "my",
              "ne",
              "nl",
              "no",
              "or",
              "pa",
              "pl",
              "ps",
              "pt",
              "ro",
              "ru",
              "sd",
              "si",
              "sl",
              "sr",
              "sv",
              "ta",
              "te",
              "th",
              "tl",
              "tr",
              "uk",
              "und",
              "ur",
              "vi",
              "zh"
            ],
            "_view_name": "DropdownView",
            "style": "IPY_MODEL_10f4526a6db7409192640b40162d326a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "DropdownModel",
            "index": 13,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0fb837787a72488395d6e5143920b5cb"
          }
        },
        "10f4526a6db7409192640b40162d326a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0fb837787a72488395d6e5143920b5cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagri32/Analyzing-the-Extent-of-Polarization-around-COVID-19-Policies-using-Social-Media/blob/main/Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfoWPoVH1nw-"
      },
      "source": [
        "\n",
        "tutorial.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1CU6bPzyd1dU9i1ye6-nuci_2WNy60zaZ\n",
        "\n",
        "# **COVID-19 Dataset - How to use it?**\n",
        "\n",
        "## ***Background***\n",
        "\n",
        "Due to the relevance of the COVID-19 global pandemic, we are releasing our dataset of tweets acquired from the Twitter Stream related to COVID-19 chatter. \n",
        "\n",
        "The data collected from the stream captures all languages, but the higher prevalence are:  English, Spanish, and French. We release all tweets and retweets on the full_dataset.tsv file, and a cleaned version with no retweets on the full_dataset-clean.tsv file .\n",
        "\n",
        "The main repository for this dataset (and latest version) can be found here https://doi.org/10.5281/zenodo.3723939\n",
        "\n",
        "Intermediate bi-weekly updates are posted here: https://github.com/thepanacealab/covid19_twitter\n",
        "\n",
        "As always, the tweets distributed here are only tweet identifiers (with date and time added) due to the terms and conditions of Twitter to re-distribute Twitter data. They need to be hydrated to be used.\n",
        "\n",
        "If you are using our dataset, please cite our preprint:\n",
        "https://arxiv.org/abs/2004.03688\n",
        "\n",
        "## ***Introduction***\n",
        "\n",
        "\n",
        "In this tutorial, we will explain in a clear and detailed way how to use the data sets generated from this repository (https://github.com/thepanacealab/covid19_twitter/tree/master/dailies). It will explain how the process of hydration of tweets is done, the process of parsing, and an example that consists of counting the unique words of a certain dataset of tweets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWNGJMucnirC"
      },
      "source": [
        "Requirements\n",
        "First, we are going to install the following modules:\n",
        "\n",
        "Twarc\n",
        "Tweepy (v. 3.8.0)\n",
        "Argparse (v 3.2)\n",
        "Xtract (v 0.1 a3)\n",
        "Wget (v 3.2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVb1n6AMnlD1"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install twarc #Twarc\n",
        "!pip install tweepy # Tweepy 3.8.0\n",
        "!pip install argparse #Argparse 3.2\n",
        "!pip install xtract #Xtract 0.1 a3\n",
        "!pip install wget #Wget 3.2\n",
        "clear_output()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yws9X2j1nntB"
      },
      "source": [
        "Selecting the dataset and language\n",
        "The dataset used for this tutorial was downloaded from here: https://github.com/thepanacealab/covid19_twitter/blob/master/dailies/2020-07-13/2020-07-13-dataset.tsv.gz?raw=true\n",
        "\n",
        "More datasets can be obtained from here: https://github.com/thepanacealab/covid19_twitter/tree/master/dailies\n",
        "\n",
        "The structure of the dataset is made up of the following fields:\n",
        "\n",
        "tweet_id The integer representation of the unique identifier for this Tweet.\n",
        "date Date when the tweet was posted (YYYY-MM-DD)\n",
        "time Time when the tweet was posted (HH:mm:ss)\n",
        "lang Language in which the text is written. Represented by a 2-character language code. If language is unknown, the value will be shown as 'und' (undefined)\n",
        "country_code Two character string representing the country where the tweet was written. If not known, the field will show as NULL\n",
        "Filtering a dataset from a language is done by specifying the language code. More information about language codes can be found here:\n",
        "\n",
        "https://developer.twitter.com/en/docs/twitter-for-websites/supported-languages\n",
        "\n",
        "In this example,we are going to filter the dataset, so we can only obtain tweets in spanish (So, that means we are going to use the language code \"es\")\n",
        "\n",
        "IMPORTANT: In this tutorial, after running the following code, please select the desired language to filter from the dropdown field (which is shown in the output code). If we don't want to filter the dataset, just select \"all\" in the dropdown field."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "77158db182fa4525bbbbd4c7572dfbe1",
            "10f4526a6db7409192640b40162d326a",
            "0fb837787a72488395d6e5143920b5cb"
          ]
        },
        "id": "PpPYxQ1aGfwv",
        "outputId": "42faa6d9-5f78-49ab-f649-8e287a8f3d15"
      },
      "source": [
        "import gzip\n",
        "import shutil\n",
        "import os\n",
        "import wget\n",
        "import csv\n",
        "import linecache\n",
        "from shutil import copyfile\n",
        "import ipywidgets as widgets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "date = \"2020-12-21\"\n",
        "dataset_URL = \"https://github.com/thepanacealab/covid19_twitter/blob/master/dailies/{}/{}-dataset.tsv.gz?raw=true\".format(date,date) #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "#Downloads the dataset (compressed in a GZ format)\n",
        "#!wget dataset_URL -O dataset.tsv.gz\n",
        "wget.download(dataset_URL, out='dataset.tsv.gz')\n",
        "\n",
        "#Unzips the dataset and gets the TSV dataset\n",
        "with gzip.open('dataset.tsv.gz', 'rb') as f_in:\n",
        "    with open('dataset-{}.tsv'.format(date), 'wb') as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "#Deletes the compressed GZ file\n",
        "os.unlink(\"dataset.tsv.gz\")\n",
        "\n",
        "#Gets all possible languages from the dataset\n",
        "df = pd.read_csv('dataset-{}.tsv'.format(date),sep=\"\\t\")\n",
        "lang_list = df.lang.unique()\n",
        "lang_list= sorted(np.append(lang_list,'all'))\n",
        "lang_picker = widgets.Dropdown(options=lang_list, value=\"en\")\n",
        "lang_picker"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77158db182fa4525bbbbd4c7572dfbe1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Dropdown(index=13, options=('all', 'am', 'ar', 'bg', 'bn', 'ca', 'ckb', 'cs', 'cy', 'da', 'de', 'dv', 'el', 'e…"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRpHl_8DnvDF"
      },
      "source": [
        "Dropdown(options=('all', 'am', 'ar', 'bg', 'bn', 'bo', 'ca', 'ckb', 'cs', 'cy', 'da', 'de', 'dv', 'el', 'en', …\n",
        "Filtering the dataset by language\n",
        "After selecting the desired language, the following code will perform the corresponding filtering to show only the records in the dataset that have the selected language (in a new tsv file called dataset-filtered.tsv). If there's no language filter, no filter process will be taken (but the file name will be dataset-filtered.tsv anyways)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RnI00fRnwfS",
        "outputId": "d9438799-9c7e-4eca-d2e9-48947e790d92"
      },
      "source": [
        "#Creates a new clean dataset with the specified language (if specified)\n",
        "filtered_language = lang_picker.value\n",
        "\n",
        "#If no language specified, it will get all records from the dataset\n",
        "if filtered_language == \"\":\n",
        "  copyfile('dataset-{}.tsv'.format(date), 'dataset-filtered-{}.tsv'.format(date)) # change all 'dataset.tsv' with 'dataset-{}.tsv'.format(date)\n",
        "\n",
        "#If language specified, it will create another tsv file with the filtered records\n",
        "else:\n",
        "  filtered_tw = list()\n",
        "  current_line = 1\n",
        "  with open('dataset-{}.tsv'.format(date)) as tsvfile:\n",
        "    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
        "\n",
        "    if current_line == 1:\n",
        "      filtered_tw.append(linecache.getline('dataset-{}.tsv'.format(date), current_line))\n",
        "\n",
        "      for line in tsvreader:\n",
        "        if line[3] == filtered_language:\n",
        "          filtered_tw.append(linecache.getline('dataset-{}.tsv'.format(date), current_line))\n",
        "        current_line += 1\n",
        "\n",
        "  print('\\033[1mShowing first 5 tweets from the filtered dataset\\033[0m')\n",
        "  print(filtered_tw[1:(6 if len(filtered_tw) > 6 else len(filtered_tw))])\n",
        "\n",
        "  with open('dataset-filtered-{}.tsv'.format(date), 'w') as f_output:\n",
        "      for item in filtered_tw:\n",
        "          f_output.write(item)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mShowing first 5 tweets from the filtered dataset\u001b[0m\n",
            "['1340885511087505408\\t2020-12-21\\t05:03:12\\ten\\tNULL\\n', '1340885511259492354\\t2020-12-21\\t05:03:12\\ten\\tNULL\\n', '1340885511557185536\\t2020-12-21\\t05:03:13\\ten\\tNULL\\n', '1340885511661998081\\t2020-12-21\\t05:03:13\\ten\\tNULL\\n', '1340885511666188289\\t2020-12-21\\t05:03:13\\ten\\tNULL\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2grRTsVnzCw"
      },
      "source": [
        "Showing first 5 tweets from the filtered dataset\n",
        "['1351757472873197568\\t2021-01-20\\t05:04:30\\tes\\tNULL\\n', '1351757472965353477\\t2021-01-20\\t05:04:30\\tes\\tNULL\\n', '1351757481723240448\\t2021-01-20\\t05:04:32\\tes\\tNULL\\n', '1351757481727455234\\t2021-01-20\\t05:04:32\\tes\\tNULL\\n', '1351757488706760705\\t2021-01-20\\t05:04:34\\tes\\tNULL\\n']\n",
        "Introducing our Twitter credentials to authenticate\n",
        "Accessing the Twitter APIs requires a set of credentials that you must pass with each request. These credentials can come in different forms depending on the type of authentication that is required by the specific endpoint that you are using. More information: https://developer.twitter.com/en/docs/apps/overview\n",
        "\n",
        "The credentials can be obtained from the developer portal (https://developer.twitter.com/en/portal/dashboard) and they look like these ones:\n",
        "\n",
        "IMPORTANT: The following code will also generate an api_keys.json (With the twitter credentials entered) that will be required later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCOfyi7yn3cv"
      },
      "source": [
        "import json\n",
        "import tweepy\n",
        "from tweepy import OAuthHandler\n",
        "\n",
        "# Authenticate\n",
        "CONSUMER_KEY = \"\" #@param {type:\"string\"}\n",
        "CONSUMER_SECRET_KEY = \"\" #@param {type:\"string\"}\n",
        "ACCESS_TOKEN_KEY = \"\" #@param {type:\"string\"}\n",
        "ACCESS_TOKEN_SECRET_KEY = \"\" #@param {type:\"string\"}\n",
        "BEARER_TOKEN_KEY = \"\"\n",
        "#Creates a JSON Files with the API credentials\n",
        "with open('api_keys.json', 'w') as outfile:\n",
        "    json.dump({\n",
        "    \"consumer_key\":CONSUMER_KEY,\n",
        "    \"consumer_secret\":CONSUMER_SECRET_KEY,\n",
        "    \"access_token\":ACCESS_TOKEN_KEY,\n",
        "    \"access_token_secret\": ACCESS_TOKEN_SECRET_KEY,\n",
        "    \"bearer_token\": BEARER_TOKEN_KEY\n",
        "     }, outfile)\n",
        "\n",
        "#The lines below are just to test if the twitter credentials are correct\n",
        "#Authenticate\n",
        "auth = tweepy.AppAuthHandler(CONSUMER_KEY, CONSUMER_SECRET_KEY)\n",
        "\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True,\n",
        "\t\t\t\t   wait_on_rate_limit_notify=True)\n",
        "\n",
        "if (not api):\n",
        "   print (\"Can't Authenticate\")\n",
        "   sys.exit(-1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t0drxH5n5S_"
      },
      "source": [
        "Hydrating the tweets (filtered dataset)\n",
        "Before parsing the dataset, an hydration process is required. In this tutorial it is done by using the following social media mining tool: https://github.com/thepanacealab/SMMT\n",
        "\n",
        "To perform this action, a python file from that repository is required (get_metadata.py) and can be obtained in the following way:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9GXnNtFzbMB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0ACCEFon6uC"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "# Get the required files from GitHub\n",
        "!wget https://raw.githubusercontent.com/cagri32/Analyzing-the-Extent-of-Polarization-around-COVID-19-Policies-using-Social-Media/main/sample-data/get_metadata.py -O get_metadata.py\n",
        "!wget https://raw.githubusercontent.com/cagri32/Analyzing-the-Extent-of-Polarization-around-COVID-19-Policies-using-Social-Media/main/main.py -O main.py\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4A4eL9mn7_X"
      },
      "source": [
        "This utility will take a file which meets the following requirements:\n",
        "*   A csv file which either contains one tweet id per line or contains at least one column of tweet ids\n",
        "*   A text file which contains one tweet id per line\n",
        "*   A tsv file which either contains one tweet id per line or contains at least one column of tweet ids\n",
        "\n",
        "For this case, the filtered dataset generated before (dataset-filtered.tsv), which is in TSV format will be used for the hydration process\n",
        "\n",
        "The arguments for this utily (get_metadata.py) are the following:\n",
        "\n",
        "*  arguments.png\n",
        "\n",
        "**PLEASE NOTE**: The -k argument refers to the json file with the Twitter credentials generated before (api_keys.json)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5b5zfgU0AYG"
      },
      "source": [
        "dataset_filtered_input = \"dataset-filtered-{}.tsv\".format(date)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzvPodrKCm5v"
      },
      "source": [
        "# tweetcount = 3000000\n",
        "# for i in range(0,tweetcount,2000):\n",
        "#   !python get_metadata.py -i {dataset-filtered-input} -o hydrated_tweets -k api_keys.json -ll {i} -ul {i+2000}\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNff2xHdoEZQ",
        "outputId": "c080840b-d46f-40e4-a5e7-8701b8d0832e"
      },
      "source": [
        "!python get_metadata.py -i {dataset_filtered_input} -o hydrated_tweets -k api_keys.json -ll 500000 -ul 1491152"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "currently getting 555200 - 555300\n",
            "currently getting 555300 - 555400\n",
            "currently getting 555400 - 555500\n",
            "currently getting 555500 - 555600\n",
            "currently getting 555600 - 555700\n",
            "currently getting 555700 - 555800\n",
            "currently getting 555800 - 555900\n",
            "currently getting 555900 - 556000\n",
            "currently getting 556000 - 556100\n",
            "currently getting 556100 - 556200\n",
            "currently getting 556200 - 556300\n",
            "currently getting 556300 - 556400\n",
            "currently getting 556400 - 556500\n",
            "currently getting 556500 - 556600\n",
            "currently getting 556600 - 556700\n",
            "currently getting 556700 - 556800\n",
            "currently getting 556800 - 556900\n",
            "currently getting 556900 - 557000\n",
            "currently getting 557000 - 557100\n",
            "currently getting 557100 - 557200\n",
            "currently getting 557200 - 557300\n",
            "currently getting 557300 - 557400\n",
            "currently getting 557400 - 557500\n",
            "currently getting 557500 - 557600\n",
            "currently getting 557600 - 557700\n",
            "currently getting 557700 - 557800\n",
            "currently getting 557800 - 557900\n",
            "currently getting 557900 - 558000\n",
            "currently getting 558000 - 558100\n",
            "currently getting 558100 - 558200\n",
            "currently getting 558200 - 558300\n",
            "currently getting 558300 - 558400\n",
            "currently getting 558400 - 558500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M57bNbVCIwqW"
      },
      "source": [
        "total ids: 1491152 for Dec 21"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWish5bJ1Lnk",
        "outputId": "e8c03102-99e3-43d1-b4c8-e2594bd9de89"
      },
      "source": [
        "!python main.py"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DiGraph with 191508 nodes and 233785 edges\n",
            "main.py... Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqnX6amg5JSX"
      },
      "source": [
        "Test if we can save a graph to a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vXHeq2C1xdZ"
      },
      "source": [
        "# import pickle\n",
        "# import networkx as nx\n",
        "\n",
        "# dg = nx.DiGraph()\n",
        "\n",
        "# # To save the graph to a file\n",
        "# pickle.dump(dg, open('/tmp/graph.txt', 'w'))\n",
        "\n",
        "# # To bring a graph from a text file\n",
        "# dg = pickle.load(open('/tmp/graph.txt'))\n",
        "\n",
        "# # Dump graph\n",
        "# with open(\"/path/to/file/multigraph.p\", 'wb') as f:\n",
        "#     pickle.dump(G, f)\n",
        "\n",
        "# # Load graph\n",
        "# with open(\"/path/to/file/multigraph.p\", 'rb') as f:  # notice the r instead of w\n",
        "#     G_loaded = pickle.load(f)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vh81dbB5Clf"
      },
      "source": [
        "Test if we can bring a graph from a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fR7yILl3E3Y",
        "outputId": "f7b5f336-e97b-4737-be03-1f1bd376d48a"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"graph.txt\", 'rb') as f:  # notice the r instead of w\n",
        "    G_loaded = pickle.load(f)\n",
        "print(G_loaded)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DiGraph with 191508 nodes and 233785 edges\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEMrJGxCHOIk"
      },
      "source": [
        "Download files and then delete"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSRx1PV6swP3"
      },
      "source": [
        "#@title Utility to zip and download a directory\n",
        "#@markdown Use this method to zip and download a directory. For ex. a TB logs \n",
        "#@markdown directory or a checkpoint(s) directory.\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "dir_to_zip = '/content' #@param {type: \"string\"}\n",
        "output_filename = \"hydrated-0-500.zip\".format()\n",
        "delete_dir_after_download = \"No\"  #@param ['Yes', 'No']\n",
        "\n",
        "os.system( \"zip -r {} {}\".format( output_filename , dir_to_zip ) )\n",
        "\n",
        "if delete_dir_after_download == \"Yes\":\n",
        "    os.system( \"rm -r {}\".format( dir_to_zip ) )\n",
        "\n",
        "files.download( output_filename )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DanQAiFR44g3"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "# files.download(\"hydrated_tweets.zip\")\n",
        "# files.download(\"hydrated_tweets.csv\")\n",
        "# files.download(\"hydrated_tweets_short.json\")\n",
        "# files.download(\"hydrated_tweets\")\n",
        "os.unlink(\"hydrated_tweets\")\n",
        "os.unlink(\"hydrated_tweets.zip\")\n",
        "os.unlink(\"hydrated_tweets.csv\")\n",
        "os.unlink(\"hydrated_tweets_short.json\")\n",
        "# os.unlink(\"graph.txt\")\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI-9saZgoIzi"
      },
      "source": [
        "From the code above, the output will generate four files:\n",
        "\n",
        "*  A hydrated_tweets.json file which contains the full json object for each of the hydrated tweets\n",
        "*  A hydrated_tweets.CSV file which contains partial fields extracted from the tweets.\n",
        "*  A hydrated_tweets.zip file which contains a zipped version of the tweets_full.json file.\n",
        "*  A hydrated_tweets_short.json which contains a shortened version of the hydrated tweets.\n",
        "\n",
        "For this tutorial, we will use the hydrated_tweets_short.json file to parse all their tweets.\n",
        "\n",
        "Parsing the tweets\n",
        "Requirements\n",
        "For this process, the following files are required and can be obtained from here:\n",
        "\n",
        "*  https://raw.githubusercontent.com/thepanacealab/SMMT/master/data_preprocessing/parse_json_lite.py\n",
        "*  https://raw.githubusercontent.com/thepanacealab/SMMT/master/data_preprocessing/fields.py\n",
        "\n",
        "Also the following modules are required:\n",
        "\n",
        "Emot\n",
        "\n",
        "Emoji"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nyu1RG7YoKFW"
      },
      "source": [
        "# from IPython.display import clear_output\n",
        "\n",
        "# !wget https://raw.githubusercontent.com/thepanacealab/SMMT/master/data_preprocessing/parse_json_lite.py -O parse_json_lite.py\n",
        "# !wget https://raw.githubusercontent.com/thepanacealab/SMMT/master/data_preprocessing/fields.py -O fields.py\n",
        "\n",
        "# !pip install emot --upgrade\n",
        "# !pip install emoji --upgrade\n",
        "\n",
        "# clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uyTspT9oLni"
      },
      "source": [
        "Pulling a sample set from the tweets\n",
        "For this tutorial, we are going to get a sample of N tweets from the hydrated dataset generated before (hydrated_tweets_short.json). The code below will generate a new JSON file (sample_data.json) with the number of samples specified.\n",
        "\n",
        "PLEASE NOTE: The code below will extract N samples from the hydrated tweets randomly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0cgCW8_0bII"
      },
      "source": [
        "**We don't need this part for our project**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmBJRURyoMWE"
      },
      "source": [
        "# import random \n",
        "\n",
        "# no_samples = \"1000\" #@param {type:\"string\"}\n",
        "# list_tweets = None\n",
        "\n",
        "# with open(\"hydrated_tweets_short.json\", \"r\") as myfile:\n",
        "#     list_tweets = list(myfile)\n",
        "\n",
        "# if int(no_samples) > len(list_tweets):\n",
        "#     no_samples = len(list_tweets)\n",
        "\n",
        "# sample = random.sample(list_tweets, int(no_samples))\n",
        "\n",
        "# file = open(\"sample_data.json\", \"w\")\n",
        "# for i in sample:\n",
        "#   file.write(i)\n",
        "# file.close() #This close() is important"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOtpKlPjoOCj"
      },
      "source": [
        "# Parsing the tweets\n",
        "\n",
        "**parse_json_lite.py**: The first argument is the json file. The second argument is optional. If the second argument is given, it will preprocess the json file. The preprocessing includes removal of URLs, twitter specific Urls, Emojis, Emoticons. The second argument (if given) must be \"p\" for the preprocessing work\n",
        "\n",
        "The following code will extract all fields in a JSON file (sample_data.json in this case). Here is a list of all available fields: https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet\n",
        "\n",
        "Keep in mind that some fields could have null or empty values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Gw088O00nQ8"
      },
      "source": [
        "**We don't need this part for our project**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgbagFjJoQgu"
      },
      "source": [
        "# !python parse_json_lite.py sample_data.json p\n",
        "\n",
        "# clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0CsYO5hoRoh"
      },
      "source": [
        "Counting the unique words from a sample set\n",
        "Given the previous sample dataset (from sample_data.json), the following code will count all the unique words (with it's frequency) in a pandas Dataframe.\n",
        "\n",
        "For practical purposes, the first 20 most used words in the dataset sample are shown in the output (this variable can be modified below)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ36owcx0r2w"
      },
      "source": [
        "**We don't need this part for our project**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybvuwnWPoSIo"
      },
      "source": [
        "# import pandas as pd\n",
        "# from collections import Counter\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# no_top_unique_words = \"20\" #@param {type:\"string\"}\n",
        "\n",
        "# df = pd.read_csv('sample_data.tsv',sep=\"\\t\")\n",
        "\n",
        "# result = Counter(\" \".join(df['text'].values.tolist()).split(\" \")).items()\n",
        "# df2 = pd.DataFrame(result)\n",
        "# df2.columns =['Word', 'Frequency']\n",
        "# df2 = df2[df2.Word != \"\"] #Deletes the empty spaces counted\n",
        "# df2 = df2.sort_values(['Frequency'], ascending=[False]) #Sort dataframe by frequency (Descending)\n",
        "\n",
        "# print('\\033[1mTop '+no_top_unique_words+' most unique words used from the dataset\\033[0m \\n')\n",
        "# print(df2.head(int(no_top_unique_words)).to_string(index=False)) #Prints the top N unique words used\n",
        "# print(\"\\n\")\n",
        "# df3 = df2.head(int(no_top_unique_words))\n",
        "# df3.plot(y='Frequency', kind='pie', labels=df3['Word'], figsize=(9, 9), autopct='%1.1f%%', title='Top '+no_top_unique_words+' most unique words used from the dataset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA4FIY2poT90"
      },
      "source": [
        "Top 20 most unique words used from the dataset \n",
        "\n",
        "     Word  Frequency\n",
        "       de        820\n",
        "       la        391\n",
        "       el        359\n",
        "       en        325\n",
        "      que        282\n",
        "        y        266\n",
        "        a        264\n",
        "      por        167\n",
        "      los        167\n",
        "      del        132\n",
        "       se        131\n",
        "      las        127\n",
        "     para        121\n",
        "      con        107\n",
        "       no        103\n",
        "   contra        100\n",
        "       un         90\n",
        "       El         90\n",
        " Covid-19         87\n",
        "       es         77\n",
        "\n",
        "\n",
        "Out[ ]:\n",
        "<matplotlib.axes._subplots.AxesSubplot at 0x7f2f7ab2e908>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7Uaz5eEAjMR"
      },
      "source": [
        "# from google.colab import files\n",
        "# !zip -r /content/file.zip /content/Folder_To_Zip\n",
        "# files.download(\"/content/file.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz7np0pkNODM"
      },
      "source": [
        "# To download a file that is created in Colab:\n",
        "\n",
        "# from google.colab import files\n",
        "# files.download(\"hydrated_tweets.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyJGHTNHq2gV"
      },
      "source": [
        "# !python get_metadata.py -i dataset-filtered.tsv -o hydrated_tweets -k api_keys.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYFxiVFkkKIt"
      },
      "source": [
        "# !python main.py"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}